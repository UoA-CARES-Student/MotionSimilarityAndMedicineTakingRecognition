# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nl2NEDRHsi7BOH8tzc0QUrYq6B6Bqu-8
"""

# To check whether GPU is available
from tensorflow.python.client import device_lib
from warnings import filterwarnings
filterwarnings('ignore')

print(device_lib.list_local_devices()) # list of DeviceAttributes

import tensorflow as tf
tf.test.is_gpu_available() # Returns true/False

# Or only check for GPU's with CUDA support
tf.test.is_gpu_available(cuda_only=True)

import pandas as pd
import numpy as np
import os, time, cv2, tqdm, datetime
import matplotlib.pyplot as plt
from tqdm import tqdm
import re

from warnings import filterwarnings
filterwarnings('ignore')

from keras import models
from keras import layers
from tensorflow.keras import optimizers

from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import ImageDataGenerator
# Compile the model
from keras.callbacks import TensorBoard
from keras.models import Model
import keras.layers as L
from tensorflow.keras.optimizers import RMSprop

# view the training log
from keras.callbacks import TensorBoard
##https://www.tensorflow.org/tensorboard/get_started



LOOK_BACK = 4
SIZE = (224,224)
num_features = 1024
#train path for each class
POSITIVES_PATH_TRAIN = '/content/drive/Shareddrives/Part IV Research Project/Medicine Action/data/Train/Class1/'
NEGATIVES_PATH_TRAIN = '/content/drive/Shareddrives/Part IV Research Project/Medicine Action/data/Train/Class2/'

POSITIVES_PATH_VALID = '/content/drive/Shareddrives/Part IV Research Project/Medicine Action/data/Val/Class1/'
NEGATIVES_PATH_VALID = '/content/drive/Shareddrives/Part IV Research Project/Medicine Action/data/Val/Class2/'


# POSITIVES_PATH_TEST = 
# NEGATIVES_PATH_TEST =



# Load the VGG model
vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE[0], SIZE[1], 3))

# Freeze the layers except the last 4 layers
for layer in vgg_conv.layers[:-4]:
    layer.trainable = False
    
# labels enabled for fine-tuning    
for layer in vgg_conv.layers:
    print(layer, layer.trainable)


 # Create the VGG16 model
def build_feat_extractor():
    model = models.Sequential()

    # Add the vgg convolutional base model
    model.add(vgg_conv)

    # Add new layers
    model.add(layers.Flatten())
    model.add(layers.Dense(1024, activation='relu'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(256, activation = 'relu'))
    model.add(layers.Dense(2, activation='softmax'))
    return model

# save the data to fit the LSTM model
def data_to_lstm_format(POSITIVES_PATH, NEGATIVES_PATH, look_back = 4):
    data = np.array([])
    labels = np.array([])
    numbers = []
    # POSITIVE LABELS
    for value in os.listdir(POSITIVES_PATH):
        numbers.append(int(re.findall(r'\d+', value.split('_')[1])[0]))
    #except ValueError:
      #pass
        #print("Positive Part: ",int(re.findall(r'\d+', value.split('_')[1])[0]))

    # filter by video
    for numb in np.unique(numbers):
        frames = []
        # append image name
        for value in os.listdir(POSITIVES_PATH):
            if int(re.findall(r'\d+', value.split('_')[1])[0]) == numb:
                frames.append(value)
        # sort image frame by frame number
        frames = sorted(frames, key = lambda x: int(re.findall(r'\d+', x.split('_')[-1].split('.')[0])[0]))
        image_data = np.zeros((len(frames), 1024))

        # get feature vector from vgg16 for each frame and stack
        for index, image in enumerate(frames):
            img = cv2.imread(POSITIVES_PATH + image)
            vect = feat_extractor.predict(img.reshape(1,224,224,3))
            image_data[index,:] = vect

        # for each frame get tensor with lookbacks
        stacked_data = np.zeros((len(frames), look_back, 1024))
        for index in range(len(frames)):
            labels = np.append(labels, [1])
            stacked_data[index, 0, :] = image_data[index]
            for lb in range(1, look_back):
                if index - lb >= 0:
                    stacked_data[index, lb, :] = image_data[index - lb]
                else:
                    stacked_data[index, lb, :] = np.zeros(1024)

        if data.shape[0] == 0:
            try:
              data = stacked_data
            except ValueError:
              pass
        else:
            try:
              data = np.concatenate([data, stacked_data], axis = 0)
            except ValueError:
              pass



    for value in os.listdir(NEGATIVES_PATH):
        #numbers.append(int(re.findall(r'\d+', value.split('_')[1])[0]))
        numbers.append(int(re.findall(r'\d+', value.split('_')[0])[0]))
      #except ValueError:
        #pass

    # filter by video
    for numb in np.unique(numbers):
        frames = []
        # append image name
        for value in os.listdir(NEGATIVES_PATH):
            #if int(re.findall(r'\d+', value.split('_')[1])[0]) == numb:
            if int(re.findall(r'\d+', value.split('_')[0])[0]) == numb:
                frames.append(value)
                #print("Negative Part2 ",re.findall(r'\d+', value.split('_')[0])[0])
        # sort image frame by frame number
        frames = sorted(frames, key = lambda x: int(re.findall(r'\d+', x.split('_')[-1].split('.')[0])[0]))
        image_data = np.zeros((len(frames), 1024))

        # get feature vector from vgg16 for each frame and stack
        for index, image in enumerate(frames):
            img = cv2.imread(NEGATIVES_PATH + image)
            vect = feat_extractor.predict(img.reshape(1,224,224,3))
            image_data[index,:] = vect

        # for each frame get tensor with lookbacks
        stacked_data = np.zeros((len(frames), look_back, 1024))
        for index in range(len(frames)):
            labels = np.append(labels, [0])
            stacked_data[index, 0, :] = image_data[index]
            for lb in range(1, look_back):
                if index - lb >= 0:
                    stacked_data[index, lb, :] = image_data[index - lb]
                else:
                    stacked_data[index, lb, :] = np.zeros(1024)

        if data.shape[0] == 0:
            try:
              data = stacked_data
            except ValueError:
              pass
        else:
            try:
              data = np.concatenate([data, stacked_data], axis = 0)
            except ValueError:
              pass
     # one hot labels
    from tensorflow.keras.utils import to_categorical
    try:
      labels = to_categorical(labels)
    except ValueError:  #raised if `y` is empty.
      pass
    return data, labels
    
# build the LSTM model
def build_model():
    inp = L.Input(shape = (LOOK_BACK, num_features))
    
    """ Use CuDNNLSTM if your machine supports CUDA
        Training time is significantly faster compared to LSTM """
    # if the compueter supports GPU and CUDA
    # you can change the LSTM to CuDNNLSTM with faster performance
    x = L.LSTM(64, return_sequences = True)(inp)
    #x = L.CuDNNLSTM(64, return_sequences = True)(inp)
    x = L.Dropout(0.2)(x)
    x = L.LSTM(16)(x)
    #x = L.CuDNNLSTM(16)(x)
    out = L.Dense(2, activation = 'softmax')(x)
    model = Model(inputs = [inp], outputs = [out])
    model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])
    return model

# view the feature extractor model's structure
build_feat_extractor().summary()

train_batchsize = 64
train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,
      width_shift_range=0.2,
      height_shift_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

#data/Train
#/content/drive/MyDrive/Action Recognition/data
train_generator = train_datagen.flow_from_directory('/medicine_taking/data/Train', class_mode='categorical', batch_size=train_batchsize, target_size = SIZE)

val_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,
      width_shift_range=0.2,
      height_shift_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

val_generator = val_datagen.flow_from_directory('/medicine_taking/data/Val/', class_mode='categorical', batch_size=train_batchsize, target_size = SIZE)



model_feat = build_feat_extractor()
model_feat.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])

# Train the model
model_feat.fit_generator(
      train_generator,  
      steps_per_epoch=train_generator.samples/train_generator.batch_size,
      validation_data = val_generator,
      validation_steps = val_generator.samples/val_generator.batch_size,
      epochs=10,
      verbose=2)
 
# Save the trained model to disk
model_feat.save('/medicine_taking/weights/Feature_Extractor.h5')


inp = model_feat.input
out = model_feat.layers[-4].output

# view the feature extractor after fine-tuning and unfreezing its top 4 layers
feat_extractor = Model(inputs = [inp], outputs = [out])
feat_extractor.summary()

feat_extractor.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])


tr_data, tr_labels = data_to_lstm_format(POSITIVES_PATH_TRAIN, NEGATIVES_PATH_TRAIN, look_back=LOOK_BACK)
val_data, val_labels = data_to_lstm_format(POSITIVES_PATH_VALID, NEGATIVES_PATH_VALID, look_back=LOOK_BACK)


# view the LSTM model's Strucutre
build_model().summary()

# save the training log
log_dir = "/medicine_taking/data/_training_logs/rnn/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=0)

model_LSTM = build_model()

history = model_LSTM.fit(tr_data, tr_labels,
                    validation_data = (val_data, val_labels),
                    callbacks = [tensorboard_callback],
                    verbose = 2, epochs = 10, batch_size = 64)

# Save the trained LSTM model weights to disk
model_LSTM.save('/medicine_taking/weights/RNN.h5')

#!pip install tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir='/content/drive/Shareddrives/Part IV Research Project/Medicine Action/data/_training_logs/rnn' --host=127.0.0.1

